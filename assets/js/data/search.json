[ { "title": "How to extract lemmatized words from a document", "url": "/posts/extract_words_from_a_document/", "categories": "", "tags": "programming, python, NLP", "date": "2021-10-26 19:30:00 +0000", "snippet": "A friend introduced to me this GitHub repo, which can extract lemmas from a document; it then compares the lemmas with the entries in a dictionary and outputs the less common lemmas that appear in the dictionary. The idea looks very nice, but I soon discovered that the stemmer does not parse the words correctly. Examples:&amp;gt;&amp;gt;&amp;gt; porter_stemmer.stem(&#39;evening&#39;)&#39;even&#39;&amp;gt;&amp;gt;&amp;gt; porter_stemmer.stem(&#39;simplified&#39;)&#39;simplifi&#39;&amp;gt;&amp;gt;&amp;gt; porter_stemmer.stem(&#39;troubles&#39;)&#39;troubl&#39;This results in a lot of words that are just wrong and thus not present in the dictionary. The Snowball stemmer is doing a better job but still not satisfying. A better strategy is to use a lemmatizer:&amp;gt;&amp;gt;&amp;gt; from nltk.stem import WordNetLemmatizer&amp;gt;&amp;gt;&amp;gt; lemmatizer = WordNetLemmatizer()&amp;gt;&amp;gt;&amp;gt; lemmatizer.lemmatize(&#39;troubles&#39;)&#39;trouble&#39;This is already much better. However, it leaves many words unchanged since the lemmatizer doesn’t know their part of speech:&amp;gt;&amp;gt;&amp;gt; lemmatizer.lemmatize(&#39;simplified&#39;)&#39;simplified&#39;&amp;gt;&amp;gt;&amp;gt; lemmatizer.lemmatize(&#39;simplified&#39;, &#39;v&#39;) # &#39;v&#39; stands for verb&#39;simplify&#39;The lemmatizer only finds the correct lemma when we explicitly say that we are processing a verb. The part-of-speech tags in a sentence can be detected automatically by some Natural Language Processing engines such as spaCy:&amp;gt;&amp;gt;&amp;gt; import spacy&amp;gt;&amp;gt;&amp;gt; nlp = spacy.load(&#39;en_core_web_sm&#39;)&amp;gt;&amp;gt;&amp;gt; sentence = &quot;I had some simplified troubles this evening.&quot;&amp;gt;&amp;gt;&amp;gt; doc = nlp(sentence)&amp;gt;&amp;gt;&amp;gt; &#39; &#39;.join([token.lemma_ for token in doc])&#39;I have some simplify trouble this evening .&#39;have -&amp;gt; had, simplified -&amp;gt; simplify, troubles -&amp;gt; trouble, all the lemmas are detected correctly. The process would be slower but more accurate than the original implementation.I hereby provide the complete script. The dependencies are textract, NLTK and spaCy. I installed the packages with Miniconda using conda install -c conda-forge spacy nltk textract. spaCy models are downloaded with spacy download en_core_web_sm. This script also includes other minor changes, such as how the punctuations are handled.filename = &#39;pride_and_prejudice.txt&#39;discard = 8000 # the first words in the dictionary to be discardeddictionary = &#39;common30k.txt&#39;import stringimport textractfrom collections import Counterfrom string import punctuationfrom nltk.tokenize import sent_tokenizefrom nltk.corpus import stopwordsimport spacyfrom tqdm import tqdmdef remove_punctuation_and_digits(from_text): # map &#39;!&quot;#$%&amp;amp;\\&#39;()*+,-./:;&amp;lt;=&amp;gt;?@[\\\\]^_`{|}~0123456789&#39; to &#39;&#39; table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation + string.digits) stripped = [w.translate(table) for w in from_text] return strippedprint(&#39;Initializing...&#39;)# Initialize spacy modelnlp = spacy.load(&#39;en_core_web_sm&#39;)print(&#39;Reading&#39;, filename, &#39;...&#39;)# read file into a list of sentencesbyte = textract.process(filename)text = byte.decode(&quot;utf-8&quot;)print(&#39;Done&#39;)print(&#39;Extracting tokens...&#39;)tokenized_text = sent_tokenize(text)tokens = [[word for word in line.split()] for line in tokenized_text]print(&#39;Done&#39;)print(&#39;Extracting lemmas...&#39;)lemmas = []for s in tqdm(tokens): doc = nlp(&#39; &#39;.join(s)) lemmas.append(remove_punctuation_and_digits( [token.lemma_ for token in doc]))print(&#39;Done&#39;)print(&#39;Sorting words...&#39;)# remove stopwords such as &#39;the&#39;, &#39;i&#39;sw = stopwords.words(&#39;english&#39;)sw.append(&#39;nt&#39;)words = [token for sentence in lemmas for token in sentence if (token.lower() not in sw and token.isalnum())]word_count = Counter(words)# favors words with higher frequencysorted_words = sorted(word_count, key=word_count.get, reverse=True)glossary = filename.split(&#39;.&#39;)[0] + &#39;_glossary.txt&#39;print(&#39;and discarding the first&#39;, discard, &#39;words in&#39;, dictionary, &#39;...&#39;)with open(dictionary) as dict: dict_words = [word for line in dict.readlines() for word in line.split()] less_common_dict_words = dict_words[discard:] new_words = [word for word in sorted_words if word in less_common_dict_words]print(&quot;Done. There are&quot;, len(new_words), &quot;potential new words&quot;)with open(glossary, &#39;w&#39;) as output: output.write(&#39;\\n&#39;.join(new_words)) print(&quot;Wrote to&quot;, glossary)spaCy can also process text in other languages, for example German:&amp;gt;&amp;gt;&amp;gt; nlp = spacy.load(&#39;de_core_news_sm&#39;)&amp;gt;&amp;gt;&amp;gt; doc = nlp(&quot;Brauner Bursche führt zum Tanze sein blauäugig schönes Kind; schlägt die Sporen keck zusammen, Csardas-Melodie beginnt; küßt und herzt sein süßes Täubchen, dreht sie, führt sie, jauchzt und springt; wirft drei blanke Silbergulden auf das Cimbal, daß es klingt.&quot;)&amp;gt;&amp;gt;&amp;gt; &#39; &#39;.join([token.lemma_ for token in doc])&#39;Brauner Bursche führen zum Tanz mein blauäugig schön Kind ; schlagen der Spore keck zusammen , Csardas-Melodie beginnen ; küssen und herzen mein süß Täubchen , drehen ich , führen ich , jauchzen und springen ; werfen drei blank Silbergulden auf der Cimbal , daß ich klingen .&#39;Reference links:Lemmatization Approaches with Examples in PythonHow to find the lemmas and frequency count of each word in list of sentences in a list?" } ]
